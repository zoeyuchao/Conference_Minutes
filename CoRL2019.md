# CoRL 2019 会议总结

<div align=center>
<img  src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/logo.png"  width="400" />
</div>

## 1.会议简介

2019年的[CoRL(Conference on Robot Learning)](https://www.robot-learning.org/)会议在日本大阪举行，时间是2019.10.29-11.1。这是第3届，第2届在瑞士苏黎世举办，我也有幸去参加了一下，但当时是个彻头彻尾的萌新，科研方向是SLAM，对于强化学习这件事根本不了解，也不知道自己要做什么，就广撒网==什么都在听。今年对于自己要做的事逐渐清晰，所以听的时候有一些侧重。

大阪要比北京暖和很多（白天有10多度度甚至20多度），温差也相对小一些，一落地就立马冬装换秋装，转了2趟地铁线，终于在天刚擦黑的时候到达了酒店（Senri Hankyu Hotel Osaka）。

### Senri Hankyu Hotel Osaka

酒店距离本次开会的地点（Senri Life Science Center）大概800m的距离，距离千里中央站（Senri Chuo Station）有5分钟步行路程。整体都不错，早晨有自助早餐（chagall餐厅）或者日本料理（Tsuruya餐厅），有可以自由交流的独立空间，如果在夏季去开会的话还可以享受室外游泳池。先上几张酒店图，个人评价还不错~
<div align=center>
<img  src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/hotel.jpg"  width="400" />
</div>

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/chagall.jpg" width="400" />
</div>

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/swim.jpg"  width="400" />
</div>

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/rizhaoshi.jpg"  width="400" />
</div>

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/buffet.jpg"  width="400"/>
</div>

### Senri Life Science Center

- 会议的地点Senri Life Science Center在新大阪，感觉比较偏，离繁华都市（比如道顿堀）有30分钟的地铁距离，开会主要集中在Senri Life Science Center的5层。

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/center.png"  width="400"/>
</div>

- 会议容量是420人 ，采取single track形式，因为人数比较多，所以组织方设置了2个大厅，2号大厅有同步直播，听和看是没有问题的，就是么得办法提问。学生注册会议会有优惠，不过去取材料的时候要记得带上学生卡，组织方需要看一下。中午会提供lunch bag，比较凉，味道还不错。每天晚上都有晚宴，不过我没去参加，去繁华都市体验了一下大阪的特色美食。
<div align=center>

<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/lunch.jpg"  width="400" />
</div>

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/lunch2.jpg"  width="400"  />
</div>

- CoRL每年都会有一个WiML(Women in Mechaine Learning)的小会议，主要面向女性，不过男性也可以参加，主要是大家互相认识一下。这次安排在第一天会议的中午，Finn大佬来做了报告，不过没有说到元学习的东西，主要是分享了自己博士课题的背景和相关的一些工作。

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/finn.jpg"  width="400"  />
</div>

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/finn2.jpg"  width="400"  />
</div>

  除了Finn做了报告之外，还有Anca Dragan，她是CoRL的常客，去年也见到她了。她的报告是无人机如何建模人并且躲避他。
<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/anca.jpg"  width="400"  />
</div>

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/anca2.jpg"  width="400"  />
</div>


## 2.文章概况

- **Acceptance rate**
  
  - 2018：237 篇文章中接收了75篇，接收率是31.6%
  - 2019：358篇文章中接收了120篇，接收率是**27.6%**
  
- **Proceedings**

  https://drive.google.com/drive/folders/17I7wD1WrKRNqaRFjEIzeNrXkA0FvHoNb

- **Program**

  https://www.robot-learning.org/home/program

- **Session**
  
  - Perception and manipulation
  - Planning and control
  - Reinforcement learning
  - Imitation learning
  - Human-robot interaction
  
- **Award**
  
  - **Best Paper Award**
    
    3B-03: A Divergence Minimization Perspective on Imitation Learning Methods
    
     (Imitation learning)
    
    Seyed Kamyar Seyed Ghasemipour, Richard Semel, Shixiang Gu 
    
  - **Best Paper Award Finalist**
    
    3F-04: Disentangled Relational Representations for Explaining and Learning from Demonstration 
    
    (Human-Robot Interaction)
    
    Yordan Hristov, Daniel Angelov, Michael Burke, Alex Lascarides, Subramanian Ramamoorthy
    
  - **Best System Paper Award**
    
    1B-03: Learning to Manipulate Object Collections Using Grounded State Representations 
    
    (Perception and manipulation)
    
    Matthew Wilson, Tucker Hermans
    
  - **Best System Paper Award Finalist**
    
    1C-07: PyRoboLearn: A Python Framework for Robot Learning Practitioners [Code](https://pypi.org/project/pyrobolearn/)
    
       (poster)
    
      Brian Delhaisse, Leonel Rozo, Darwin G. Caldwell
    
  - **Best Presentation Award**
  
    2B-02: Bayesian Optimization Meets Riemannian Manifolds in Robot Learning
  
     (Reinforcement learning)
    
    Noemie Jaquier, Leonel Rozo, Sylvain Calinon, Mathias Burger

## 3.文章列表

整理了一下跟自己课题组相关的方向，题目先甩上来。发现一个特点，基本每篇文章都有附件，要么是视频，要么是代码，视频偏多一些。这可能也反映了作者的态度吧，以后投文章也要记得拍个视频出来，毕竟做机器人的，没有实物等于白说。

- 多机
  - Connectivity Guaranteed Multi-robot Navigation via Deep Reinforcement Learning
  - Predictive Safety Network for Resource-constrained Multi-agent Systems
  - PIC: Permutation Invariant Critic for Multi-Agent Deep Reinforcement Learning
  - Macro-Action-Based Deep Multi-Agent Reinforcement Learning
  - Multi-Agent Reinforcement Learning with Multi-Step Generative Models
  - Learning from My Partner's Actions: Roles in Decentralized Robot Teams
  - 图神经网络
    - Learning Decentralized Controllers for Robot Swarms with Graph Neural Networks
    - Graph Policy Gradients for Large Scale Robot Control

- 导航
  - 视觉 导航
    - Learning to Navigate Using Mid-level Visual Priors
    - Combining Optimal Control and Learning for Visual Navigation in Novel Environments
    - Learning Navigation Subroutines from Egocentric Videos
  - 语言-视觉 导航
    - Conditional Driving from Natural Language Instructions
    - Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight
    - Vision-and-Dialog Navigation
    - Language-guided Semantic Mapping and Mobile Manipulation in Partially Observable Environments

- 元学习
  - MAME: Model-Agnostic Meta-Exploration
  - Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning [Code](https://meta-world.github.io/)
- 在线学习
  - An Online Learning Procedure for Feedback Linearization Control without Torque Measurements
  - Model-based planning with energy based models
  - On-Policy Robot Imitation Learning from a Converging Supervisor
  - Data Efficient Reinforcement Learning for Legged Robots
- Sim-to-Real
  - TuneNet: One-Shot Residual Tuning for System Identification and Sim-to-Real Robot Task Transfer
  - Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real

- 图神经网络
  - Graph-Structured Visual Imitation

## 4.文章解析

### MAME：Model-Agnostic Meta-Exploration

[照片](https://github.com/zoeyuchao/Conference_Minutes/tree/master/figure/CoRL2019/MAME)

**作者**：Swaminathan Gurumurthy Sumit Kumar Katia Sycara

**组织**：Robotics Institute, Carnegie Mellon University (CMU)

**内容**：

### Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning [Code](https://meta-world.github.io/)
**作者**： Tianhe Yu, Deirdre Quillen, Zhanpeng He], Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine

**组织**：UC berkeley

**内容**：

- multi-task reinforcement learning的目的是学习**一个**policy，能够更有效地处理很多tasks，并且要比单独地学习task要好（promise: learn s single policy that can solve multiple tasks more efficiency than learning the tasks individually）。现在的multi-task RL的benchmark有DM Lab和Atari，有两个缺点：
  - 都是游戏设定，缺少实际应用（limited to game setting and lack of realistic use cases）
  - 迁移到不相关的游戏上不好使（little effciency to be gained on disjoint games）
- meta-RL的目的是利用以往的experiences有效地学习新的tasks（promise: efficiently acquire new tasks by leveraging experiences from past tasks）。现在的Meta-RL的benchmark就是mujoco那些设定，有3个缺点：
  - 任务的分布非常限制（task distributions are very narrow）
  - 适应的是**相同**任务的新变化而已（adaptation to new variations of the same task）
  - 称之为“multi-goal” benchmark更合适（better characterized as “multi-goal” benchmarks）
- 所以本文的目标是使meta-RL能够泛化新的不同的技能上并且去评价泛化性能（goal：enable meta-RL to generalize to new distinct skills and evaluate the generalization performance）
  - large, diverse task set-->generalization to new tasks
- Meta-world的独特之处在于：一个有很多task的多任务和元强化的数据集，为了研究元强化学习如何加速新任务的学习（A new multi-task and meta-RL benchmark with a wide range of tasks to study how meta-RL accelerates acquisition of new tasks），
  - 还是抓取数据集。（50 robotics manipulation tasks）
  - 用5种不同的模式评价meta-RL算法。（on five different modes）

### PyRoboLearn: A Python Framework for Robot Learning Practitioners [Code](https://pypi.org/project/pyrobolearn/)

**作者**：Brian Delhaisse, Leonel Rozo, Darwin G. Caldwell

**内容**：这是一篇推广文，介绍python的新库pyrobolearn。机器人有很多仿真环境，比如：ROS,GAZEBO,MuJoCo,**Bullet,RBDL**(mark<!--没用过，记得查一下-->)，仿真环境的语言是C++>Matlab>Python;机器学习的种类特别多，框架就是tensorflow和pytorch，再加个gym框架，语言是Python>C++>Matlab。那么，大一统思想自然而然出现了。PyRoboLearn从ROS订阅一些话题进各种simulator->对应world和robot->定义state和action->创建出env，定义policy->recorder（类似于跑一段数据？）和interface（传感器）->定义task。然后就开始训练和测试。overview和示例使用如下图所示：

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/pyrobolearn/overview.jpg"  width="400"  />
</div>

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/pyrobolearn/case.jpg"  width="400"  />
</div>

### Model-based planning with energy based models

**作者**：

**组织**：

**内容**：

### Data Efficient Reinforcement Learning for Legged Robots

**作者**：

**组织**：

**内容**：

### Learning Decentralized Controllers for Robot Swarms with Graph Neural Networks

**作者**：

**组织**：

**内容**：

### Connectivity Guaranteed Multi-robot Navigation via Deep Reinforcement Learning [Video](https://drive.google.com/open?id=1l0cD3tauGbIinRnm0_vWyRiODkR2KYR9)

**作者**： Juntong Lin, Xuyun Yang, Peiwei zheng, Hui Cheng

**组织**：中山大学

**内容**：大概的任务是让多个机器人能够保持一定的连接（欧氏距离小于某个阈值），提出了一个CSPF（constraint satisfying parametric function），仿真环境用的是Virtual policy extended environment (VP2E) ，跟我们的好像是差不多的，然后在实际环境上也做了实验(setup跟我们很像，也是optitrack来做全局定位，车载处理器也是TX2，不过我们的车更小，他的车是robomaster，有点大。)，不过感觉场景比较简单，任务也比较简单，场地很小，机器人跑几步就到达目标了，算法的优势在实际环境中并不明显。

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/VP2E.jpg"  width="400"  />
</div>

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/realworld.jpg"  width="400"  />
</div>

### Macro-Action-Based Deep Multi-Agent Reinforcement Learning

**作者**：

**组织**：

**内容**：


## 5.Keynote

### Kenji Doya
**主题**: Reinforcement learning in Meachines and the Brain

**组织**：Okinawa Institute Science of Technology 

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/kenji/kenji.jpg"  width="400"  />
</div>

**内容**：脑科学和强化学习是共同发展的，上一张总结图，有一个工作是多巴胺作为reward在猴子身上做实验。

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/kenji/0.JPG"  width="400"  />
</div>

首先一部分工作是用强化学习来做机器人的控制，他们组实际上很早就开始做机器人控制（比如185年的learn to walk和2001年的learn to stand up，以及他的学生Pavvo Parmas用强化学习做自平衡车 learn to Bounce up and balance，感觉他们的工作都是在线学习，用TD Error直接做的），他总结了一个model-based和model-free RL的区别，model-free RL在真实机器人上很难用，主要是慢。

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/kenji/2.JPG"  width="400"  />
</div>
on-policy 稳定但是样本效率低，off-policy 样本效率高但是不稳定，所以有actor-critic。
<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/kenji/3.JPG"  width="400"  />
</div>

然后他们组还在研究强化学习和脑结构的一些关系，1999年发表了文章，认为每一个脑结构都完成了特殊的功能，比如监督学习，强化学习和无监督学习，看起来他们是在小鼠和猴子身上做实验。

<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/kenji/4.jpg"  width="400"  />
</div>

最后提出了自动化AI的danger：
<div align=center>
<img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/kenji/1.jpg"  width="400"  />
</div>

## Chelsea Finn 

**主题**：如何加快学习

**内容**：Finn从自己的博士课题由来讲起，举了一个学习的例子，机器人从0开始在1个环境中学习1个任务（learn one task in one environment, starting from the scratch）,但是人做的工作比机器人要多得多（捡球放球，收集数据），这样搞下去是不可行的（rely on detailed supervision and guidance）。如果我们想要造一个能够理解世界并且与世界交互的机器人，那么我们需要从起点（outset）再好好思考一下。放了一个动图：婴儿在屋子里滚来滚去自己玩各种玩具，引出几个关键词：Can we learn **reusable models** from ==raw sensor inputs== in **`diverse environments`** with *minimal supervision*?

- 实验室之间共享数据集：step1: Collect a dataset step2: evaluate if it's useful (RoboNet那个工作)

## Anca Dragan

**主题**：How to assume people are（approximately）optimal and get away with it

**内容**：对人的行为进行建模，任务是：一架无人机要飞过一个门，需要躲避一个人。人如果按照某个行为一直走，无人机检测出来以后，可以进行建模，然后修正自己的路线，绕开这个人。这时候地上突然撒了一杯咖啡，人走着走着就需要改变自己的路线，绕开这杯咖啡，（noisy-rationality: too narrow）无人机要继续能work的话就需要做一些操作：

- how do you leverage the model when it's right, but become conservative when it's wrong?

- 加一个rationality coefficient $\beta$ ,来estimate apparent rationality.物理意义是如果发现人的行为跟模型不符合，对模型保持怀疑（If the human appears too soboptimal to the model, be skeptical of the model）。

  <div align=center>
  <img src="https://github.com/zoeyuchao/Conference_Minutes/blob/master/figure/CoRL2019/beta.jpg"  width="400"  />
  </div>

- human rationality = model confidence

- mind the gap （real human behavior and noisy rationality assumption）